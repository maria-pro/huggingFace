# 1.3.3. Getting started with the datasets library

### Getting started with the datasets library

<mark style="background-color:purple;">Need:</mark> <mark style="background-color:purple;"></mark><mark style="background-color:purple;">`https://huggingface.co/datasets/stanfordnlp/imdb`</mark>&#x20;

Welcome back!  Let's dig into coding: data and datasets library!

Let's start our second Hugging Face project.&#x20;

{% hint style="info" %}
<mark style="color:purple;">What we will cover:</mark>

* <mark style="color:purple;">install the library</mark>
* <mark style="color:purple;">load datasets, and</mark>
* <mark style="color:purple;">perform basic steps such as exploring and filtering data.</mark>
{% endhint %}

&#x20;<mark style="color:green;">In the previous video we started with datasets at Hugging Face, talked about the ML data flow and started our second project. Now - more coding!</mark>

in this project we will work with one of the most remarkable datasets at Hugging Face: `imdb` dataset [here](https://huggingface.co/datasets/stanfordnlp/imdb) . The IMDB dataset is a movie reviews data,  sourced from the Internet Movie Database (IMDB) and contains 50,000 highly polarized reviews.

Let's go to the Hugging Face website and have a look at its dataset card.

We can have a quick look at the dataset even without loading it using **Dataset Viewer** and read about the dataset under the [Dataset Summary ](https://huggingface.co/datasets/stanfordnlp/imdb#dataset-card-for-imdb). We see that the dataset includes two columns: text and label.

As you see on the `Splits` view, the dataset is evenly split into 25,000 reviews for training and 25,000 reviews for testing, with each set having an equal number of positive and negative reviews. Hoorah, our dataset is "balanced", which is not that common thing in the world of data.

Each review typically includes:

* **Review Text**: The main body of the review written by the user.
* **Sentiment Label**: A binary label indicating the sentiment of the review (positive or negative).

The size of the dataset is 50,000 reviews, which is manageable and comprehensive for NLP tasks.

But let's go to Google Colab and start coding.

We need to install the `datasets` library first and ensure that we have integration with \```huggingface_hub`,`` which we set up earlier.

```python
!pip install datasets 
!pip install huggingface_hub
```

We will use load\_dataset function to get our data straight from Hugging Face. There are also other ways to load data using this library, we will look at them later.

```python
from datasets import load_dataset 
dataset=load_dataset("imdb")
print(dataset)
```

We have training and testing splits in our data - which are subsets of data we can use for training and testing. So let's look at them

```python
train_dataset=dataset['train']
test_dataset=dataset['test']
print(train_dataset)
print(test_dataset)
```

We also can load just a particular split that we are interested in using

```
train_dataset = load_dataset("imdb", split="train")
```

and by the way - if you are running this code locally and want to check where this dataset goes on your computer, we can check the cache location: this is one features of the `dataset` library.

```python
from datasets import config

print(config.HF_DATASETS_CACHE)
```

This will print the location of the cache directory. In our case we are using Google Colab so it shows:

```python
/root/.cache/huggingface/datasets
```

Let's move on and look at the structure of the `train_dataset`.&#x20;

```python
train_dataset.column_names
```

```
train_dataset.features
```

as you see we have two categories of the `label` that is negative coded as 0 and positive ones coded as 1.

So when to use which of those attributes?&#x20;

Notice that `column_names` returns a list of strings with where each string is the name of a column.&#x20;

`features` returns a dictionary where the keys are the column names and the values describe the type and other properties of each column. &#x20;

I usually use `column_names`to to quickly see the structure of the dataset or use column names in the code.

Notice that we cannot use either of these attributes on the full dataset.

```python
dataset.column_names
dataset.features
```

&#x20;This is because the dataset object itself can contain multiple splits, and each can have different structures. We will talk more about the dataset objects later.

We can look at individual observations in the dataset to get a fill of data:

```python
train_dataset[0]
```

which returns the first observations, get a range of observations using

```python
train_dataset[range(20)]
```

what do you think we are going to get if we pull up `train_dataset[-1]`? Yes, the last observation.

We can also use a column name to get a list of values in that column:

<pre class="language-python"><code class="lang-python"><strong>train_dataset["text"]
</strong></code></pre>

If we need, we can rename columns using

```python
dataset_new = train_dataset.rename_column("text", "review")
```

Again, we cannot do this on the full dataset since it is a `DatasetDict` object.

Lastly, let's have a look at filtering. We can do this using the `filter` method with a `lambda` function, such as&#x20;

retrieve positive reviews or filtering reviews with a specific movie title (and looking at first 3 of them)

<pre class="language-python"><code class="lang-python"># Filter positive reviews (label == 1) from the training dataset
<strong>positive_reviews = train_dataset.filter(lambda x: x['label'] == 1)
</strong>print(positive_reviews[:5])

# Filter reviews containing a specific movie title (e.g., 'Inception')
movie_title_reviews = train_dataset.filter(lambda x: 'inception' in x['text'].lower())
print(movie_title_reviews[:3])  #
</code></pre>

Great! so now we know how to&#x20;

* install the `dataset`library
* load datasets from `HuggingFace`
* start exploring our dataset
* and do some basic transformations with it

<mark style="color:purple;">**Let's move to the next video!**</mark>
