# 1.2.2. Sneak peak at models

### Sneak peak at models: Model Hub

{% hint style="info" %}
What we will cover:

* Understand the basic usage of Hugging Face models.
* Run simple code examples to see the models in action.
* Get hands-on experience with different types of models.
{% endhint %}

Let's have a look at some examples of ML tasks where we can use models. Let's do a combination of very different tasks to see the power of Hugging face:

* Text-related task (usually referred to as Natural Language processing or NLP): we will try to summarise a text
* image-related task, such as image classification. This is part of the Computer Vision (CV) domain
* and audio task: text-to-speech generation.

These three are very common tasks in AI. Let's first try a simple example: we will explore the backend of these tasks and model later in the course, but for now - let's try them!

First, lets look at text.&#x20;

#### NLP and Text Summarization

Text-related tasks are very common in every day life:

* Just think about that thick book on your bookshelf that you wanted to start reading ages ago and running out of time. Would be awesome to have a summary of it in a few seconds! That's text summarization!
* or think about your morning news scroll when you go through news headers to see if it is worth a read? That is text classification!&#x20;
* or writing an email to your boss or a friend to tell them a great news that you are doing "Hugging Face" bootcamp now? :) that is text generation!

We will talk more about NLP  later, but in this mini project let's have a look at summarizing quite a lengthy piece of text - when we distill the most important information into a concise summary. <mark style="color:blue;">Hint: next time you build your news aggregation site - say thank you!</mark>

####

* Understand the basic usage of Hugging Face models.
* Run simple code examples to see the models in action.
* Get hands-on experience with different types of models.

#### Session Breakdown

**Part 1: BART for Text Summarization (NLP)**

**Steps:**

1.  **Install Necessary Libraries**:

    ```python
    !pip install transformers
    ```
2.  **Load the Summarization Pipeline**:

    ```python
    from transformers import pipeline

    # Load the summarization pipeline
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    ```
3.  **Prepare Input Text**:

    ```python
    # Sample text
    text = """
    The Hugging Face Transformers library is a powerful tool for working with state-of-the-art natural language processing models. 
    It provides an easy-to-use interface for loading and fine-tuning models for various NLP tasks such as text classification, 
    named entity recognition, question answering, and text summarization. With a large community of users and contributors, 
    Hugging Face continues to be at the forefront of NLP research and application development.
    """
    ```
4.  **Perform Summarization**:

    ```python
    # Perform summarization
    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)

    # Print the summary
    print(summary[0]['summary_text'])
    ```

**Part 2: Vision Transformer (ViT) for Image Classification (Computer Vision)**

**Steps:**

1.  **Install Necessary Libraries**:

    ```python
    !pip install transformers
    !pip install torch
    !pip install torchvision
    ```
2.  **Load the ViT Model and Preprocess Image**:

    ```python
    from transformers import ViTFeatureExtractor, ViTForImageClassification
    from PIL import Image
    import requests

    # Load pre-trained model and feature extractor
    model_name = "google/vit-base-patch16-224"
    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
    model = ViTForImageClassification.from_pretrained(model_name)

    # Load and preprocess image
    url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tasks/image_classification/woman.png"
    image = Image.open(requests.get(url, stream=True).raw)
    inputs = feature_extractor(images=image, return_tensors="pt")
    ```
3.  **Perform Inference**:

    ```python
    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Get predicted class
    predicted_class = logits.argmax(-1).item()
    print(f"Predicted class: {model.config.id2label[predicted_class]}")
    ```

**Part 3: CLIP for Multimodal Tasks**

**Steps:**

1.  **Load the CLIP Model and Tokenizer**:

    ```python
    from transformers import CLIPProcessor, CLIPModel
    from PIL import Image
    import requests

    # Load pre-trained model and processor
    model_name = "openai/clip-vit-base-patch16"
    model = CLIPModel.from_pretrained(model_name)
    processor = CLIPProcessor.from_pretrained(model_name)

    # Load and preprocess image
    url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tasks/image_classification/woman.png"
    image = Image.open(requests.get(url, stream=True).raw)

    # Prepare input text
    text = ["a photo of a woman", "a photo of a man"]
    inputs = processor(text=text, images=image, return_tensors="pt", padding=True)
    ```
2.  **Perform Inference**:

    ```python
    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
        probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the probabilities

    # Get predicted label
    predicted_label = probs.argmax(-1).item()
    print(f"Predicted label: {text[predicted_label]}")
    ```

#### Summary

In this session, we have demonstrated how to use Hugging Face models for different tasks: NLP with BART for text summarization, Computer Vision with ViT for image classification, and Multimodal tasks with CLIP. Each example showed how to load a model, preprocess input data, and perform inference in Google Colab.

#### Additional Resources

* [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)
* [Google Colab](https://colab.research.google.com/)

This breakdown and the code examples will help you quickly get started with these powerful models and understand their practical applications.

#### Part 3: Text-to-Speech (TTS)

For this example, we'll use the `facebook/fastspeech2-en-ljspeech` model for text-to-speech conversion.

**Steps:**

1.  **Install Necessary Libraries**:

    ```python
    !pip install transformers
    !pip install torch
    !pip install torchaudio
    !pip install soundfile
    ```
2.  **Load the TTS Model and Processor**:

    ```python
    from transformers import Wav2Vec2Processor, HubertForCTC
    import torchaudio
    import soundfile as sf

    # Load pre-trained processor and model
    processor = Wav2Vec2Processor.from_pretrained("facebook/fastspeech2-en-ljspeech")
    model = HubertForCTC.from_pretrained("facebook/fastspeech2-en-ljspeech")
    ```
3.  **Prepare Input Text**:

    ```python
    # Sample text
    text = "Hugging Face Transformers library makes working with NLP models easy."
    ```
4.  **Convert Text to Speech**:

    ```python
    # Convert text to speech
    inputs = processor(text, return_tensors="pt", padding=True)
    speech = model.generate(inputs.input_ids, max_length=100, num_beams=5, early_stopping=True)

    # Save the speech to a file
    sf.write('output.wav', speech.numpy(), 22050)

    # Optionally, play the audio in Colab
    import IPython.display as ipd
    ipd.Audio('output.wav')
    ```

#### Complete Example

Here's the complete code for converting text to speech using a pre-trained TTS model in Google Colab:

```python
# Install Necessary Libraries
!pip install transformers
!pip install torch
!pip install torchaudio
!pip install soundfile

# Load the TTS Model and Processor
from transformers import Wav2Vec2Processor, HubertForCTC
import torchaudio
import soundfile as sf

# Load pre-trained processor and model
processor = Wav2Vec2Processor.from_pretrained("facebook/fastspeech2-en-ljspeech")
model = HubertForCTC.from_pretrained("facebook/fastspeech2-en-ljspeech")

# Prepare Input Text
text = "Hugging Face Transformers library makes working with NLP models easy."

# Convert Text to Speech
inputs = processor(text, return_tensors="pt", padding=True)
speech = model.generate(inputs.input_ids, max_length=100, num_beams=5, early_stopping=True)

# Save the speech to a file
sf.write('output.wav', speech.numpy(), 22050)

# Optionally, play the audio in Colab
import IPython.display as ipd
ipd.Audio('output.wav')
```

#### Summary

In this session, we have demonstrated how to use Hugging Face models for different tasks: NLP with BART for text summarization, Computer Vision with ViT for image classification, and Text-to-Speech (TTS) using a pre-trained model. Each example showed how to load a model, preprocess input data, and perform inference in Google Colab.

#### Additional Resources

* [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)
* [Google Colab](https://colab.research.google.com/)

This breakdown and the code examples will help you quickly get started with these powerful models and understand their practical applications.



