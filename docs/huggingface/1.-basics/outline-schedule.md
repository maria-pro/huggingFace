---
description: breakdown of the course and progression
---

# Outline schedule

### Module 1.2 Models

<table data-full-width="true"><thead><tr><th width="240">Meet the HF models</th><th width="221">Notes</th><th data-type="checkbox">Script</th><th data-type="checkbox">Video</th><th>Code<select><option value="EM9LGN7qDODu" label="todo" color="blue"></option><option value="z9doSyyNRC49" label="done" color="blue"></option></select></th><th>id</th></tr></thead><tbody><tr><td>HF Hub</td><td></td><td>true</td><td>false</td><td></td><td>1.2.1</td></tr><tr><td>Sneak peak at models: Model Hub</td><td>may split into 3</td><td>true</td><td>false</td><td></td><td>1.2.2</td></tr><tr><td>Intro to transformer models</td><td></td><td>false</td><td>false</td><td></td><td>1.2.3</td></tr><tr><td>Tasks and models</td><td></td><td>false</td><td>false</td><td></td><td>1.2.4</td></tr><tr><td>Pretrained models: what does it mean</td><td></td><td>false</td><td>false</td><td></td><td>1.2.5</td></tr><tr><td>Selecting a model</td><td></td><td>false</td><td>false</td><td></td><td>1.2.6</td></tr><tr><td>Reading the model card</td><td></td><td>false</td><td>false</td><td></td><td>1.2.7</td></tr><tr><td>Trending models guide</td><td></td><td>false</td><td>false</td><td></td><td>1.2.8</td></tr><tr><td>Using models for inference</td><td></td><td>false</td><td>false</td><td></td><td>1.2.9</td></tr><tr><td>Downloading models</td><td></td><td>false</td><td>false</td><td></td><td>1.2.10</td></tr><tr><td>Model outputs and interpretation</td><td></td><td>false</td><td>false</td><td></td><td>1.2.11</td></tr><tr><td>Customizing model parameters</td><td></td><td>false</td><td>false</td><td></td><td>1.2.12</td></tr><tr><td>Model widgets</td><td></td><td>false</td><td>false</td><td></td><td>1.2.13</td></tr><tr><td>Understanding model limitations</td><td></td><td>false</td><td>false</td><td></td><td>1.2.14</td></tr><tr><td>Putting this all together: workflow for working with any pretrained models</td><td></td><td>false</td><td>false</td><td></td><td>1.2.15</td></tr><tr><td>Questions to ask before starting with a model</td><td></td><td>false</td><td>false</td><td></td><td>1.2.16</td></tr></tbody></table>

### Module 1.3. Datasets

<table data-full-width="true"><thead><tr><th>Meet the HF datasets</th><th>Notes</th><th width="110" data-type="checkbox">Script</th><th width="125" data-type="checkbox">Video</th><th width="111">Code<select><option value="DCzTXqSr4XA2" label="todo" color="blue"></option><option value="oDTc3pjDY2P5" label="done" color="blue"></option></select></th><th>id</th></tr></thead><tbody><tr><td>Data: the start of it all</td><td></td><td>false</td><td>false</td><td></td><td>1.3.1</td></tr><tr><td>Sneak peak at datasets</td><td>HF hub<br>Dataset card<br>Copyrights and licences</td><td>true</td><td>false</td><td></td><td>1.3.2</td></tr><tr><td>Getting started with the datasets library</td><td>install datasets library<br>load_dataset<br>locate entries (indexing, slicing)<br>basic filtering<br>filtering with lambda functions<br>renaming columns</td><td>true</td><td>false</td><td><span data-option="DCzTXqSr4XA2">todo</span></td><td>1.3.3</td></tr><tr><td>Datasets ü§ù Arrow</td><td><p>Caching vs loading: benefits</p><p>Speed comparison<br><a href="https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a></p></td><td>false</td><td>false</td><td><span data-option="DCzTXqSr4XA2">todo</span></td><td>1.3.4</td></tr><tr><td>Most common datasets for tasks</td><td>Datasets for NLP<br>CV<br>audio<br>what's next after loading,<br>tasks in data prepping</td><td>false</td><td>false</td><td><span data-option="DCzTXqSr4XA2">todo</span></td><td>1.3.5</td></tr><tr><td>Exploring dataset structures</td><td>preview without loading <br>DatasetDic<br></td><td>false</td><td>false</td><td><span data-option="DCzTXqSr4XA2">todo</span></td><td>1.3.6</td></tr><tr><td>Dataset splits and formats</td><td>why splits<br>custom datasets with no splits<br>batches<br></td><td>false</td><td>false</td><td></td><td>1.3.7</td></tr><tr><td>Data filtering and processing</td><td>filtering vs selecting<br>indexing vs slicing</td><td>false</td><td>false</td><td></td><td>1.3.8</td></tr><tr><td>Advanced data manipulations</td><td></td><td>false</td><td>false</td><td></td><td>1.3.9</td></tr><tr><td>Batching and iterating</td><td>interatable datasets vs noniteratable</td><td>false</td><td>false</td><td></td><td>1.3.10</td></tr><tr><td>Loading custom datasets</td><td></td><td>false</td><td>false</td><td></td><td>1.3.11</td></tr><tr><td>Streaming datasets</td><td></td><td>false</td><td>false</td><td></td><td>1.3.12</td></tr><tr><td>Questions to ask before starting with a dataset</td><td></td><td>false</td><td>false</td><td></td><td>1.3.13</td></tr></tbody></table>



### Module 1.4. Meet transformers

<table><thead><tr><th>Meet transformers</th><th>Notes</th><th width="40" data-type="checkbox">Script</th><th data-type="checkbox">Video</th><th data-type="checkbox">Code</th><th>id</th></tr></thead><tbody><tr><td>What are transformers?</td><td></td><td>false</td><td>false</td><td>false</td><td>1.4.1</td></tr><tr><td>Why transformers? and what was before them</td><td></td><td>false</td><td>false</td><td>false</td><td>1.4.2</td></tr><tr><td><mark style="background-color:purple;">Additional: transformers library vs pytorch vs tensorflow</mark></td><td></td><td>false</td><td>false</td><td>false</td><td>1.4.2.1</td></tr><tr><td>Attention is all you need! where it all started</td><td></td><td>false</td><td>false</td><td>false</td><td>1.4.3</td></tr><tr><td>What do we need to know about the attention mechanism</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Key concepts behind transformers: attention heads, layers, positional encoding.</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Attention heads</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Layers</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Positional encoding</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Simplified explanation of self-attention and its advantages over RNNs and CNNs.</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Architecture of a basic transformer model</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Encoders and decoders explained</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>How data flows through a transformer model</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Visual diagrams to illustrate model components</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Understanding pre-trained transformer models</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Examples of pre-trained models: BERT and GPT-2</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>General applications of these models</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Step-by-step guide to loading a pre-trained model using Hugging Face</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Basic fine-tuning concepts</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Why fine-tuning?</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Basic steps to fine-tune a transformer model on a new dataset.</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Overview of parameters that commonly need adjustments.</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Simple demo</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Transformer models in practice</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td>Conclusion</td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr><tr><td></td><td></td><td>false</td><td>false</td><td>false</td><td></td></tr></tbody></table>



### Sneak peak at datasets



{% hint style="info" %}
<mark style="background-color:purple;">What we will cover:</mark>

* <mark style="background-color:purple;">how to install the library</mark>
* <mark style="background-color:purple;">load datasets, and</mark>
* <mark style="background-color:purple;">perform basic operations such as exploring and filtering data.</mark>
{% endhint %}

***

> #### Problem 1
>
> **I need data. Can to get my data in Hugging Face?**&#x20;
>
> Data is the start of it all: it defines everything you do in your project. Hugging Face is amazing place as has lot of datasets that we can use to jump start building great things AI.



> #### Problem 2
>
> **I found my data, but how can I work with it using Hugging Face tools?**
>
> What you most commonly want to do with the data is to load it, get some information about your data structure and setup, prep your data by doing some common transformation of your data.&#x20;
>
> To summariselo:
>
> data
>
> * load data
> * get info about your data
> * how to prep your data for processing

***





Let's have a closer look at datasets at HF and start working with `datasets` library.

We can find datasets at HF [here](https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdatasets) and we can get all the documentation for the library here

We are going to work very close with this library, so bookmark the link!  The library is amazing and simplifies how to access and work with myriads of datasets for an ML/AI project.



Now that you are all setup, the first step is to load a dataset. The easiest way to load a dataset is from the [Hugging Face Hub](https://huggingface.co/datasets). There are already over 900 datasets in over 100 languages on the Hub. Choose from a wide category of datasets to use for NLP tasks like question answering, summarization, machine translation, and language modeling. For a more in-depth look inside a dataset, use the live [Datasets Viewer](https://huggingface.co/datasets/viewer/).

#### 1. Installation

First, ensure you have the `datasets` library installed. If not, you can install it using pip.

`! pip install datasets`

#### Do:

* [ ] Open your Google Collab and use the code above to install the \`datasets\` library.
* [ ] Verify the installation by importing the library with

`import datasets`

#### 2. Loading a Dataset



{% hint style="info" %}
Make sure to check the previous section and setup your API key in GoogleColab to prevent annoying warning
{% endhint %}

We'll start by loading a dataset from the Hugging Face Hub. For this example, let's use the `imdb` dataset. Lets's see where we can find this dataset on the website first!

```
from¬†datasets¬†import¬†load_dataset

#¬†Load¬†the¬†IMDB¬†dataset
dataset¬†=¬†load_dataset('imdb')
```

#### Hands-on Exercise 2

1. In a Python script or Jupyter notebook, load the `imdb` dataset using the code above.
2. Print the loaded dataset to see its structure.

#### 3. Exploring the Dataset (10 minutes)

Once the dataset is loaded, we can explore its contents. This includes viewing the dataset's structure, columns, and a few examples.

```
#¬†View¬†the¬†dataset
print(dataset)

#¬†Access¬†a¬†specific¬†split
train_dataset¬†=¬†dataset['train']

#¬†Check¬†the¬†structure
print(train_dataset.column_names)
print(train_dataset.features)
print(train_dataset[0])
```

#### Hands-on Exercise 3

1. Access the `train` split of the `imdb` dataset.
2. Print the column names and features of the `train` split.
3. Print the first example in the `train` split.

#### 4. Filtering and Selecting Data (10 minutes)

Filtering and selecting data is crucial for preparing datasets for training models. We'll learn how to filter the dataset based on specific conditions.

```
#¬†Filter¬†the¬†dataset¬†to¬†only¬†include¬†positive¬†reviews
positive_reviews¬†=¬†train_dataset.filter(lambda¬†x:¬†x['label']¬†==¬†1)

#¬†Select¬†a¬†subset¬†of¬†columns
subset¬†=¬†train_dataset.select(range(1000))
```

#### Hands-on Exercise 4

1. Filter the `train` split of the `imdb` dataset to include only positive reviews.
2. Select the first 1000 entries from the `train` split.
3. Print a few examples from the filtered and selected datasets.
